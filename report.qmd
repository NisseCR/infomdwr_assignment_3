---
title: "Supervised learning competition"
author: 
  - Nisse Hermsen
  - Isabelle de Wolf
  - Sara Sakhi
  - Celis Tittse
  - Agnieszka Kubica
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
bibliography: references.bib  
---

```{r}
#| label: Load dependencies
#| echo: false
#| warning: false
#| message: false

# Data manipulation
library(tidyverse)
# Data visualisation
library(ggplot2)
# Cross-validation methods
library(caret)
# Cross association
library(GGally)
# Correlation graph
library(reshape2)
# Missing data
library(ggmice)
# RF model
library(randomForest)
# Gridding of plots
library(gridExtra)
# Make a nice looking table
library(kableExtra)
```

```{r}
#| label: Load data
#| echo: false

data_train <- readRDS("data/train.rds") %>% as_tibble()
data_test <- readRDS("data/test.rds") %>% as_tibble()
```

```{r}
#| label: Project setup
#| echo: false

# Set set for reproducibility
set.seed(123)
```

# Data description

Describe the data and use a visualization to support your story. (approx. one or two paragraphs)


The data used to test the prediction model contains 79 observations with 30 predictor variables. The test data will not be explored any further in the EDA, to ensure no bias will be introduced when working with the training data. The data used to train the prediction model consists of 316 observations of the outcome variable `score` and 30 predictor variables. The table below summarizes information about each of these variables. Importantly, the degree of the data is rather large compared to the cardinality.  Additionally, `Medu` and `Fedu` are found to be of ordinal nature, with the distance between the numbers decoding categories not being meaningful. Thus, representing these variables as an ordered factor is more fitting, rather than numerical variables as in the original metadata. Lastly, the numerical variables exhibit a variety of scales.

```{r}
#| code-summary: "Variable descriptions"

descs <- read.csv("./data/variable_description.csv", header = TRUE, sep = ";") %>%
  as_tibble()

args <- c("striped", "scale_down", "hold_position", "repeat_header")

kbl(descs, booktabs = T) %>%
    kable_styling(latex_options = args) 

```


TODO: Too many variables narrative main problems:
- curse of dimentionality - distance for knn becomes less meaningful
- overfitting because of many variables and not too many rows

```{r}
#| label: EDA visualisation histogram
#| code-summary: "Create histograms"
#| warning: false
#| message: false

make_hist <- function(col) {
  ggplot(data_train, aes_string(x = col)) +
    geom_histogram() +
    theme_minimal()
}


grid.arrange(
  make_hist("age"),
  make_hist("Medu"),
  make_hist("Fedu"),
  make_hist("traveltime"),
  make_hist("studytime"),
  make_hist("failures"),
  make_hist("famrel"),
  make_hist("freetime"),
  make_hist("goout"),
  make_hist("Dalc"),
  make_hist("Walc"),
  make_hist("health"),
  make_hist("absences"),
  make_hist("score"),
  nrow = 3
)
```

```{r}
#| label: EDA visualisation barplot
#| code-summary: "Create barplots"
#| warning: false
#| message: false

make_bar <- function(col) {
   ggplot(data_train, aes(x = fct_infreq(as.factor(data_train[[col]])))) +
    geom_bar(width = 0.8) +
    theme_minimal() +
    xlab(col) +
    coord_flip() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    scale_y_continuous(expand = expansion(mult = c(0.1, 0.3)))
    
}


grid.arrange(
  make_bar("school"),
  make_bar("sex"),
  make_bar("address"),
  make_bar("famsize"),
  make_bar("Pstatus"),
  make_bar("Mjob"),
  make_bar("Fjob"),
  make_bar("reason"),
  make_bar("guardian"),
  make_bar("schoolsup"),
  make_bar("famsup"),
  make_bar("paid"),
  make_bar("activities"),
  make_bar("nursery"),
  make_bar("higher"),
  make_bar("internet"),
  make_bar("romantic"), 
  nrow = 4
)
```
Looking at the variables, non really stand out. numerical variables are a bit skewed, but this might be due to the variable descriptions. For the categorical ones, some of them seemed a bit skewed as well, but no obvious flaws or errors. There does not seem to be any intentionally disguised missing data. 

## Missing data
```{r}
#| label: Missing data
#| code-fold: false
#| code-summary: "Missing data"

anyNA(data_train)
```

The dataset contains no missing data. Moreover, disguised missing values—such as `0`, `-999` or `"None"`—seems absent as well when investigating the data using the provided distributions for the predictor data.


```{r}
#| label: Correlation matrix
#| echo: false
#| warning: false
#| message: false

# select only numerical variables train dataset
num_data <- dplyr::select_if(data_train, is.numeric)

# the following code was taken from http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization 

# make the graph for  fully observed dataset:

# compute the correlations across variables
cormatrix <- cor(num_data, use="complete.obs") %>% round(digits = 2)

# take only half of the correlation matrix to not have repeated values
get_lower_tri<-function(cormatrix){
    cormatrix[upper.tri(cormatrix)] <- NA
    return(cormatrix)
  }
lower_tri_c <- get_lower_tri(cormatrix)

# removes the NA values
melted_cormatrix_c <- melt(lower_tri_c, na.rm = TRUE)

# make the tile graph
cortile_c <- ggplot(melted_cormatrix_c, aes(y = Var2, x = Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 12, hjust = 1), axis.text.y  = element_text(
    size = 12))+
 coord_fixed()

print(cortile_c)
```
The correlation plots reveals relatively weak or lack of correlations between the variables. Score is most strongly correlated with failures, the relationship is negative. The variables absences, freetime and famrel reveal very little correlation with score. This may suggest that they are not the best predictors. However, there is a chance that once controlled for other variables the predictive power of the variables increases, thus, correlation is not enough information for feature selection.

# Data transformation and pre-processing

Describe additional pre-processing steps you have used, if any (e.g., dealing with categorical data, scaling). If you do not do any pre-processing, you can leave this section out.


Why Likert scales can remain numerical:  source: [@stats_sol]

---

In addition to casting the data to the correct datatypes, the continuous predictors are transformed using a z-score standardisation. This transformation is required for one of the models (KNN), since each feature should be weighted equally when looking for neighbouring datapoints and their respective distances. To facilitate MSE comparison between the models, predictor data is standardised for all the models by scaling the data prior to the training process. That said, standardisation is applied separately to the `data_train` and `data_test` sets to prevent "data leakage", which would cause target information to be available during the training process whilst being unavailable when actually making predictions.

```{r}
#| label: preprocessing
#| warning: false
#| code-fold: false
#| code-summary: "Preprocessing"

# Cast variables to factor based on EDA.
factorise <- c("Medu", "Fedu")
data_train <- data_train %>% mutate_at(vars(factorise), as.factor) 
data_test <- data_test %>% mutate_at(vars(factorise), as.factor) 

# Scale the predictor data.
data_train <- data_train %>%
  mutate(across(.cols = !score & where(is.numeric), scale))

data_test <- data_test %>%
  mutate(across(where(is.numeric), scale))
```



# Model description

For this assignment, three models will be compared: a linear regression model with automatic feature selection, a KNN model with a custom k-value, and a random forest model. K-fold cross validation will be used to ensure fair comparison between the models, whilst also providing an averaged Mean Squared Error (MSE) to facilitate this comparison. Thus, MSE is the performance metric of these models. It should be noted that models are cross validated even when it makes less sense for the model to do so. This is specifically true for the random forest model, as this model already tries to minimize overfitting in the building process. However, applying cross-validation to this model will yield validation results in a similar format as the other models, thus allowing for easier comparison. 

The linear regression model will be build using backward stepwise feature selection. This will be done to reduce the model's dimensionality, and thus reduce the variance of the model by taking into account the model complexity. The stepwise function aims to reduce the AIC error of the model, which is an estimator of prediction error that includes a penalty for using more parameters / predictors. The KNN model will be built using the `tunelength` parameter. By supplying a value to this parameter, multiple k-values will be used to compute KNN models. Of these KNN models, the model with the lowest average MSE over all the cross validation folds will be chosen as the final model. Lastly the random forest model will also be cross validated, as mentioned before. The final random forest model will be the one where the number of predictors chosen for each tree is optimized, meaning the model with the number of parameters in each tree model where the MSE will be the lowest. 

# Model creation
We first set up the folds we want to use for the k-fold cross validation. We decided to set `k` to 10, thus performing the training and validation of models 10 times over different combinations of data. Each time the model was validated on approximately 30 cases and train on the remaining data. We found that this data ratio as sufficient. To ensure comparability of performance of the models, the seed was set to value `123` before each training algorithm. The seed guarantees that the algorithm is performed on the same data partitions between each model [@Brownlee_2019].


```{r}
#| label: Train configuration
#| code-fold: false
#| code-summary: "Train configuration"

# Set up repeated k-fold cross-validation configuration.
train_control <- trainControl(method = "cv", number = 10)
```

```{r}
#| label: Linear regression model
#| code-summary: "Create linear regression model"

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the linear regression model.
model_lm <- train(
  score ~ .,
  data = data_train,
  method = "lmStepAIC",
  trace = FALSE, 
  direction = "backward",
  trControl = train_control,
)
```


```{r}
#| label: KNN model
#| code-summary: "Create KNN model"

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the KNN model.
model_knn <- train(
  score ~ .,  
  data = data_train,
  method = "knn",
  tuneLength = 10, 
  trControl = train_control
)
```

```{r}
#| label: Random forest model
#| code-summary: "Create random forest model"

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the random forest model.
model_rf <- train(
  score ~ .,
  data = data_train,
  method = "rf",
  trControl = train_control
)
```



# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

```{r}
#| label: Model comparison
#| code-fold: false
#| code-summary: "Compare the models"

# Resampling + comparison method taken from https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/
results <- resamples(list(LM=model_lm, KNN=model_knn, RF=model_rf))
summary(results)
bwplot(results)
dotplot(results)

# TODO create a custom ggplot that is both aligned (X-axis ticks), faceted and obviously **pretty**.
# Do so with the data.frame data provided in results$values.
head(results$values)

# TODO Maybe also consider variable importance, might have surprising insights?
# e.g. varImp(model_knn, scale=FALSE) (code doesn't work yet)

#Sara is working
```

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

Finally, the data to calculate the expected mean squared error is provided by using this model to make predictions on the test set `data_test`. These predictions are

```{r}
#| label: Predict test
#| code-summary: "Make predictions on test data"

# Make predictions
predictions <- predict(model_rf, data_test)

# Save predictions
write_rds(predictions, "data/predictions.rds")
```

# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d

# References

<div id="refs"></div>
