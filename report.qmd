---
title: "Supervised learning competition"
author: 
  - Nisse Hermsen
  - Isabelle de Wolf
  - Sara Sakhi
  - Celis Tittse
  - Agnieszka Kubica
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

# Data manipulation
library(tidyverse)
# Data visualisation
library(ggplot2)
# Cross-validation methods
library(caret)
# Cross association
library(GGally)

library(randomForest)
```

```{r}
#| label: data loading
#| echo: false

data_train <- readRDS("data/train.rds") %>% as_tibble()
data_test <- readRDS("data/test.rds") %>% as_tibble()
```

```{r}
#| label: setup
#| echo: false

# Set set for reproducibility
set.seed(123)
```

# Data description

Describe the data and use a visualization to support your story. (approx. one or two paragraphs)

TODO:
- var table from csv
- hists for each var (cat vs cont)
- correlation w/ score (what vars are corr, predict outcome in feature selection)
- mention prevention of "leakage" by only using `data_train` in EDA.
- check for outliers?

```{r}
#| label: eda visualization

# TODO add EDA plots.
# TODO add variable table.
# TODO consider using few of points from Peng's list (slide) e.g. checking "n's".
```

```{r}
#| label: preprocessing

# TODO decide whether we should preprocess before or after EDA visualisations. After means working with values that are less interpretable.

# Scaling should be done when using KNN (distance based algorithm),
# since distance is sensitive to scale of variables: https://medium.com/codex/why-scaling-your-data-is-important-1aff95ca97a2
# Scaling is done per data split (train and test) to prevent leakage.

# Exclude outcome "score" during scaling due to immutable benchmark test data.
data_train <- data_train %>%
  mutate(across(.cols = !score & where(is.numeric), scale))

data_test <- data_test %>%
  mutate(across(where(is.numeric), scale))
```

# Model description

Briefly describe which models you compare to perform prediction. (approx. two or three paragraphs)

# Data transformation and pre-processing

Describe additional pre-processing steps you have used, if any (e.g., dealing with categorical data, scaling). If you do not do any pre-processing, you can leave this section out.

# Model creation

"The random number seed is set before each algorithm is trained to ensure that each algorithm gets the same data partitions and repeats. This allows us to compare apples to apples in the final results" - from [here](https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/).

```{r}
#| label: train configuration

# Set up repeated k-fold cross-validation configuration.
train_control <- trainControl(method = "cv", number = 10)
```

```{r}
#| label: linear regression model

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the linear regression model.
model_lm <- train(
  score ~ .,
  data = data_train,
  method = "lmStepAIC",
  trace = FALSE, 
  direction = "backward",
  trControl = train_control,
)
```


```{r}
#| label: knn model

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the KNN model.
model_knn <- train(
  score ~ .,  # TODO remove highly correlated features based on EDA
  data = data_train,
  method = "knn",
  tuneLength = 10, 
  trControl = train_control
)
```

```{r}
#| label: random forest model

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the random forest model.
model_rf <- train(
  score ~ .,
  data = data_train,
  method = "rf",
  trControl = train_control
)
```


# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

```{r}
#| label: model comparison

# Resampling + comparison method taken from https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/
results <- resamples(list(LM=model_lm, KNN=model_knn, RF=model_rf))
summary(results)
bwplot(results)
dotplot(results)

# TODO create a custom ggplot that is both aligned (X-axis ticks), faceted and obviously **pretty**.
# Do so with the data.frame data provided in results$values.
head(results$values)

# TODO Maybe also consider variable importance, might have surprising insights?
# e.g.
varImp(model_knn, scale=FALSE)
```

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
