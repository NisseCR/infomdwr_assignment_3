---
title: "Supervised learning competition"
author: 
  - Nisse Hermsen
  - Isabelle de Wolf
  - Sara Sakhi
  - Celis Tittse
  - Agnieszka Kubica
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
---

```{r}
#| label: R packages
#| echo: false
#| warning: false
#| message: false

# Data manipulation
library(tidyverse)
# Data visualisation
library(ggplot2)
# Cross-validation methods
library(caret)
# Cross association
library(GGally)
# correlation graph
library(reshape2)

library(randomForest)
```

```{r}
#| label: data loading
#| echo: false

data_train <- readRDS("data/train.rds") %>% as_tibble()
data_test <- readRDS("data/test.rds") %>% as_tibble()
```

```{r}
#| label: setup
#| echo: false

# Set set for reproducibility
set.seed(123)
```

# Data description

Describe the data and use a visualization to support your story. (approx. one or two paragraphs)

TODO:
- var table from csv
- hists for each var (cat vs cont)
- correlation w/ score (what vars are corr, predict outcome in feature selection)
- mention prevention of "leakage" by only using `data_train` in EDA.
- check for outliers?

```{r}
#| label: eda visualization

# TODO add EDA plots.
# TODO add variable table.
# TODO consider using few of points from Peng's list (slide) e.g. checking "n's".
```

```{r tile-cor}

# select only numerical variables train dataset
num_data <- dplyr::select_if(data_train, is.numeric)

# the following code was taken from http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization 

# make the graph for  fully observed dataset:

# compute the correlations across variables
cormatrix <- cor(num_data, use="complete.obs") %>% round(digits = 2)

# take only half of the correlation matrix to not have repeated values
get_lower_tri<-function(cormatrix){
    cormatrix[upper.tri(cormatrix)] <- NA
    return(cormatrix)
  }
lower_tri_c <- get_lower_tri(cormatrix)

# removes the NA values
melted_cormatrix_c <- melt(lower_tri_c, na.rm = TRUE)

# make the tile graph
cortile_c <- ggplot(melted_cormatrix_c, aes(y = Var2, x = Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 12, hjust = 1), axis.text.y  = element_text(
    size = 12))+
 coord_fixed()

print(cortile_c)
```
The correlation plots reveals relatively weak or lack of correlations between the variables. Score is most strongly correlated with failures, the relationship is negative. The variables absences, freetime and famrel reveal very little correlation with score. This may suggest that they are not the best predictors. However, there is a chance that once controlled for other variables the predictive power of the variables increases, thus, correlation is not enough information for feature selection. 



```{r}
#| label: preprocessing

# TODO decide whether we should preprocess before or after EDA visualisations. After means working with values that are less interpretable.

# Scaling should be done when using KNN (distance based algorithm),
# since distance is sensitive to scale of variables: https://medium.com/codex/why-scaling-your-data-is-important-1aff95ca97a2
# Scaling is done per data split (train and test) to prevent leakage.
# Exclude outcome "score" during scaling due to immutable benchmark test data.
# TODO scaling can also be done using caret functions as described here: https://rpubs.com/njvijay/16444
data_train <- data_train %>%
  mutate(across(.cols = !score & where(is.numeric), scale))

data_test <- data_test %>%
  mutate(across(where(is.numeric), scale))

summary(data_train)
```

# Model description

Briefly describe which models you compare to perform prediction. (approx. two or three paragraphs)

# Data transformation and pre-processing

Describe additional pre-processing steps you have used, if any (e.g., dealing with categorical data, scaling). If you do not do any pre-processing, you can leave this section out.

# Model creation

"The random number seed is set before each algorithm is trained to ensure that each algorithm gets the same data partitions and repeats. This allows us to compare apples to apples in the final results" - from [here](https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/).

```{r}
#| label: train configuration

# Set up repeated k-fold cross-validation configuration.
train_control <- trainControl(method = "cv", number = 10)
```

```{r}
#| label: linear regression model

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the linear regression model.
model_lm <- train(
  score ~ .,
  data = data_train,
  method = "lmStepAIC",
  trace = FALSE, 
  direction = "backward",
  trControl = train_control,
)
```


```{r}
#| label: knn model

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the KNN model.
model_knn <- train(
  score ~ .,  # TODO remove highly correlated features based on EDA
  data = data_train,
  method = "knn",
  tuneLength = 10, 
  trControl = train_control
)
```

```{r}
#| label: random forest model

# Set seed before initialising train_control to create consistent folds across
# the models (https://stackoverflow.com/questions/52622811/does-using-the-same-traincontrol-object-for-cross-validation-when-training-multi).
set.seed(123)

# Create the random forest model.
model_rf <- train(
  score ~ .,
  data = data_train,
  method = "rf",
  trControl = train_control
)
```


# Model comparison

Describe how you compare the methods and why. (approx. two or three paragraphs)

```{r}
#| label: model comparison

# Resampling + comparison method taken from https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/
results <- resamples(list(LM=model_lm, KNN=model_knn, RF=model_rf))
summary(results)
bwplot(results)
dotplot(results)

# TODO create a custom ggplot that is both aligned (X-axis ticks), faceted and obviously **pretty**.
# Do so with the data.frame data provided in results$values.
head(results$values)

# TODO Maybe also consider variable importance, might have surprising insights?
# e.g. varImp(model_knn, scale=FALSE) (code doesn't work yet)
```

# Chosen model

Show which method is best and why. (approx. one paragraph) You are welcome to use tables and plots!

```{r}
#| label: table example
data.frame(
  model       = c("Cool model 1", "Cool model 2"),
  performance = c(1.2, 1.8),
  other       = c(0.5, 0.3),
  notes       = c("Some note", "another note")
)
```

# Team member contributions

Write down what each team member contributed to the project.

- Author One: a, b, c
- Author Two: b, c, d
- Author Three: a, b, d
