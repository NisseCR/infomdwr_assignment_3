---
title: "Supervised learning competition"
author: 
  - Nisse Hermsen
  - Isabelle de Wolf
  - Sara Sakhi
  - Celis Tittse
  - Agnieszka Kubica
date: last-modified
format:
  html:
    toc: true
    self-contained: true
    code-fold: true
    df-print: kable
bibliography: references.bib  
---

```{r}
#| label: Load dependencies
#| echo: false
#| warning: false
#| message: false

# Data manipulation
library(tidyverse)
# Data visualisation
library(ggplot2)
# Cross-validation methods
library(caret)
# Cross association
library(GGally)
# Correlation graph
library(reshape2)
# Missing data
library(ggmice)
# RF model
library(randomForest)
# Gridding of plots
library(gridExtra)
# Make a nice looking table
library(kableExtra)
```

```{r}
#| label: Load data
#| echo: false

data_train <- readRDS("data/train.rds") %>% as_tibble()
data_test <- readRDS("data/test.rds") %>% as_tibble()
```

```{r}
#| label: Project setup
#| echo: false

# Set set for reproducibility
set.seed(123)
```

# Data description

The dataset consist of records from two Portuguese high schools and includes both student marks on a mathematics course and a questionnaire concerned with the background of students, their personal life, and their study habits. The data from @Original_pap research on the influence of the demographical, social, and school-related factors on students' performance. The original dataset also included the performance of a Portuguese course with an additional 649 observations. However, those will not be included in this assignment.

@Original_pap used linear regression, neural networks, Decision Trees, Support Vector Machines, and Random Forest in their analysis [@Original_pap, p. 12]. This study will replicate their findings with linear regression, k-nearest neighbours, and Random Forest, with the intention to find the best performing prediction model on the test dataset. Other literature suggests that the background and social factors influence school performance of teenagers. Therefore, it is valid to perform a prediction model on this dataset and expect relatively good performance [@Original_pap, p. 12].    

The data used to train the prediction model consists of 316 observations of the outcome variable `score` and 30 predictor variables. The table below summarises information about each of these variables. Importantly, the degree of the data is rather large compared to the cardinality. The high dimensionality (30 features) relative to the sample size (316 observations) results in two key challenges. First, the curse of dimensionality makes distance-based methods, including k-Nearest Neighbors (KNN), less effective, as distances become less meaningful in high-dimensional spaces. Second, the large number of variables increases the risk of overfitting, where models capture noise rather than meaningful patterns due to insufficient data relative to the number of features. The model can become overly complex, capturing noise in the training data rather than true patterns, which leads to poor performance on unseen data. Feature selection is crucial to mitigate these issues and improve model performance.

Apart from high dimensionality, the Variable Descriptions table reveals presence of multiple ordinal variables that represent their categories with numbers. Examples include: `Medu`, `studytime` and `freetime`. The numerical representation means that the variables are treated as if continuous by the models, which is a problem since the distances between the categories of such variables are meaningless. It may lead to a worse performance of linear regression, as its coefficients interpretation is based on unit increases and assumption of constant change over those step increases. Recasting those variables into ordered factors and, thus, using dummy encoding for the linear modeling should be considered. 

Additionally, the numerical variables exhibit a variety of scales. This is a problem for the KNN model, since variables with larger scales become more important in distance based models. Thus, normalization of numerical variables is required.  

```{r}
#| code-summary: "Variable descriptions"

descs <- read.csv("./data/variable_description.csv", header = TRUE, sep = ";") %>%
  as_tibble()

args <- c("striped", "scale_down", "hold_position", "repeat_header")

kbl(descs, booktabs = T) %>%
    kable_styling(latex_options = args) 

```

The data used to test the prediction model contains 79 observations with 30 predictor variables. The test data will not be explored any further in the EDA, to ensure no bias will be introduced when working with the training data.  

## Variable distributions

```{r}
#| label: EDA visualisation histogram
#| code-summary: "Create histograms"
#| warning: false
#| message: false

make_hist <- function(col) {
  ggplot(data_train, aes_string(x = col)) +
    geom_histogram(fill = "steelblue", color = "black", alpha = 0.8) +
    labs(x = col, y = "Frequency") +
    theme_minimal() +
    theme(
      panel.grid.major = element_line(size = 0.5, linetype = 'dashed', color = 'gray'),
      panel.grid.minor = element_blank()
    )
}


grid.arrange(
  make_hist("age"),
  make_hist("Medu"),
  make_hist("Fedu"),
  make_hist("traveltime"),
  make_hist("studytime"),
  make_hist("failures"),
  make_hist("famrel"),
  make_hist("freetime"),
  make_hist("goout"),
  make_hist("Dalc"),
  make_hist("Walc"),
  make_hist("health"),
  make_hist("absences"),
  make_hist("score"),
  nrow = 3
)
```

```{r}
#| label: EDA visualisation barplot
#| code-summary: "Create barplots"
#| warning: false
#| message: false

make_bar <- function(col) {
   ggplot(data_train, aes(x = fct_infreq(as.factor(data_train[[col]])))) +
    geom_bar(width = 0.8, fill = "steelblue", color = "black", alpha = 0.8) +
    theme_minimal() +
    labs(x = col, y = "Frequency") +
    coord_flip() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    scale_y_continuous(expand = expansion(mult = c(0.1, 0.3))) +
    theme(
      panel.grid.major = element_line(size = 0.5, linetype = 'dashed', color = 'gray'),
      panel.grid.minor = element_blank()
    )
    
}


grid.arrange(
  make_bar("school"),
  make_bar("sex"),
  make_bar("address"),
  make_bar("famsize"),
  make_bar("Pstatus"),
  make_bar("Mjob"),
  make_bar("Fjob"),
  make_bar("reason"),
  make_bar("guardian"),
  make_bar("schoolsup"),
  make_bar("famsup"),
  make_bar("paid"),
  make_bar("activities"),
  make_bar("nursery"),
  make_bar("higher"),
  make_bar("internet"),
  make_bar("romantic"), 
  nrow = 4
)
```

Looking at the histograms and bar plots, none of the variables stand out as particularly unusual. The numerical variables show some skewness, which might be expected given their descriptions. For some numerical variables it's doubtful whether they actually represent continuous values, as their disjointed distributions seem more like ordered variables. This is consistent with the variable descriptions. For the categorical variables, there are a few that seem somewhat imbalanced, but nothing that suggests any clear issues or data quality concerns. Overall, there don’t appear to be any obvious errors in the distributions, and the plots show no signs of intentionally hidden missing data either.

## Correlations 

```{r}
#| label: Correlation matrix
#| echo: false
#| warning: false
#| message: false

# select only numerical variables train dataset
num_data <- dplyr::select_if(data_train, is.numeric)

# the following code was taken from http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization 

# make the graph for  fully observed dataset:

# compute the correlations across variables
cormatrix <- cor(num_data, use="complete.obs") %>% round(digits = 2)

# take only half of the correlation matrix to not have repeated values
get_lower_tri<-function(cormatrix){
    cormatrix[upper.tri(cormatrix)] <- NA
    return(cormatrix)
  }
lower_tri_c <- get_lower_tri(cormatrix)

# removes the NA values
melted_cormatrix_c <- melt(lower_tri_c, na.rm = TRUE)

# make the tile graph
cortile_c <- ggplot(melted_cormatrix_c, aes(y = Var2, x = Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 12, hjust = 1), axis.text.y  = element_text(
    size = 12))+
 coord_fixed()

print(cortile_c)
```

The correlation plots reveals relatively weak or lack of correlations between the variables. Score is most strongly correlated with failures, the relationship is negative. The variables `absences`, `freetime` and `famrel` reveal very little correlation with score. This may suggest that they are not the best predictors. However, there is a chance that once controlled for other variables the predictive power of the variables increases. Therefore, low correlation is not enough information for feature selection, however, it supports the noticed need for some form of it. 

## Missing data
```{r}
#| label: Missing data
#| code-fold: false
#| code-summary: "Missing data"

anyNA(data_train)
```

The dataset contains no missing data. Moreover, disguised missing values—such as `0`, `-999` or `"None"`— seems absent as well when investigating the data using the provided distributions for the predictor data. The only exception is `Medu` and `Fedu`, where there is a small number of `0` entries, that could represent no education or missing values. However, the proper representation is assumed. 

# Data transformation and pre-processing

In the EDA it has been discovered that some of the variables are not necessarily correctly labeled as numerical. Particularly, `Medu`, `Fedu`, `studytime`, `traveltime` and a variety of "numerical" variables using 5 point Likert scales are ordinal variables. That is because the the distance between each of their categories is not meaningful. However, studies have shown that for ordinal variables with 5 or more categories "ordinal approximation of a continuous variable"  does not negatively impact the analysis [Statistics Solutions, @stats_sol]. Effectively, the variables can be treated as continuous in the modelling process without negative consequences for the analysis. A decision is made to leave `Medu`, `Fedu` and the 5 point Likert scale variables as numerical. However, `studytime` and `traveltime` who have only 4 categories are cast to ordered factors and are not being approximated as continuous. 

In addition to casting the data to the correct datatypes, the continuous predictors are transformed using a z-score standardisation. This transformation is required for one of the models (KNN), since each feature should be weighted equally when looking for neighbouring datapoints and their respective distances. To facilitate RMSE comparison between the models, predictor data is standardised for all the models by scaling the data prior to the training process. That said, standardisation is applied separately to the `data_train` and `data_test` sets to prevent "data leakage", which would cause target information to be available during the training process whilst being unavailable when actually making predictions.

```{r}
#| label: preprocessing
#| warning: false
#| code-fold: false
#| code-summary: "Preprocessing"

# Cast variables to factor based on EDA.
factorise <- c("studytime", "traveltime")
data_train <- data_train %>% mutate_at(vars(factorise), as.factor) 
data_test <- data_test %>% mutate_at(vars(factorise), as.factor) 

# Scale the predictor data.
data_train <- data_train %>%
  mutate(across(.cols = !score & where(is.numeric), scale))

data_test <- data_test %>%
  mutate(across(where(is.numeric), scale))
```



# Modelling and comparison strategy 

For this assignment, three models will be compared: a linear regression model with automatic feature selection, a KNN model with a custom k-value, and a random forest model. All models will be train on the train data within k-fold cross validation to reduce the chances of overfitting.

## Linear regression
The linear regression model will be build using backward stepwise feature selection. This will be done to reduce the model's dimensionality, and thus reduce the variance of the model by taking into account the model complexity. The stepwise function aims to reduce the AIC error of the model, which is an estimator of prediction error that includes a penalty for using more parameters / predictors. Backwards stepwise feature selection is an algorithm that starts the modeling with all features included in the formula. Then it removes a single feature that leads to the largest AIC reduction in each step. When no AIC reduction can be achived, the current model is chosen as the best one. The backwards selection is chosen, as opposed to best-subset or forward selection, because the former is computationally expensive and the latter may not arrive at the optimal model. 

## KNN model
Tuning the number of nearest neighbors (`k`) allows for the KNN model to optimize the bias and variance tradeoff. To do so, the KNN model will be built using the `tunelength` parameter. By supplying a value to this parameter, multiple k-values will be used to compute KNN models. Of these KNN models, the model with the lowest average RMSE over all the cross validation folds will be chosen as the final model. 

## Random forest
The random forest model already tries to minimize overfitting in the building process by sampling both data points and features across different trees and averaging the results. However, it will also be k-fold cross validated, as mentioned before. While this step is not necessary for reducing the variance of the model, it is required for fair comparison across all of the trained models. The final random forest model will be the one where the number of predictors chosen for each tree is optimized, meaning the model with the number of parameters in each tree model where the RMSE will be the lowest. 

## Comparison 
The comparison of the models will be based on their performance in the K-fold cross validation. K-fold was chosen as opposed to a train and validation data split, as it reduces the chance that one of the models performed particularly well because a the validation data was particularly well suited for it, but performance on other splits would be much worse. In other words, it is a better guarantee of not overfitting the models and identifying ones who's performance is consistently good. To facilitate this comparison, the Root Mean Squared Error (RMSE) was chosen as the the main performance metric. For RMSE scores to be a fair comparison metric, the models have to be trained and tested on folds consisting of the same data points. That is the reason the random forest model will also be included in the K-fold procedure. To guarantee the algorithm is performed on the same data partitions between each model, the seed can be set right before each of the cross-validation processes [@Brownlee_2019].


# Model creation

The training controle is a cross-validation k-fold method. The configuration is initialised with the number of folds (`k`) set to 10, thus performing the training and validation of models 10 times over different combinations of data. Each time the model was validated on approximately 30 cases and train on the remaining data. It was found that this data ratio was sufficient. To ensure comparability of the model performance, the seed was set to value `123` before each training algorithm.

The LmStepAIC is explained earlier in the modelling strategy section. Furthermore, only the final model is of interest, thus the `trace` parameter is set to `False` within the `train()` function. The LM with all the variables has a high complexity in this model, and this method reduces the complexity by removing the variables with low impact (based on AIC). Backwards direction was chosen to reduce a complex model to a simpler model.

The KNN model has little hyperparameters. Besides the implementation of cross-validation, the KNN model will select the score of its closest neighest neighboures based on the variables in the dataset. This method will be applied for to `k` values 5 to 23, with increments of 2, thus creating 10 `k` instances er per the `tuneLength` parameter that was set to 10.

```{r}
#| label: Train configuration
#| code-fold: false
#| code-summary: "Train configuration"

# Set up repeated k-fold cross-validation configuration.
train_control <- trainControl(method = "cv", number = 10)
```

```{r}
#| label: Linear regression model
#| code-summary: "Create linear regression model"

# Set seed before initialising train_control to create consistent folds across models
set.seed(123)

# Create the linear regression model.
model_lm <- train(
  score ~ ., # specifying the initial formula: score predicted by all other variables, no interaction terms
  data = data_train,
  method = "lmStepAIC",  # linear regression with step feature selection based on AIC 
  trace = FALSE, 
  direction = "backward", # specifying backwards feature selection
  trControl = train_control, # cross validation configuration 
)
```


```{r}
#| label: KNN model
#| code-summary: "Create KNN model"

# Set seed before initialising train_control to create consistent folds across models
set.seed(123)

# Create the KNN model.
model_knn <- train(
  score ~ .,  # specifying the initial formula: score predicted by all other variables
  data = data_train,
  method = "knn",
  tuneLength = 10, 
  trControl = train_control
)
```

```{r}
#| label: Random forest model
#| code-summary: "Create random forest model"

# Set seed before initialising train_control to create consistent folds across models
set.seed(123)

# Create the random forest model.
model_rf <- train(
  score ~ .,
  data = data_train,
  method = "rf",
  trControl = train_control
)
```



# Model comparison

Resampling the model results per fold gives insight into the variation of the model's RMSE and MAE, by feeding this data into the `bwplot()` function.

```{r}
#| label: Model comparison
#| code-summary: "Create performance metrix boxplot"
#| message: false

# Resampling + comparison method taken from https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/
results <- resamples(list(LM=model_lm, KNN=model_knn, RF=model_rf))
bwplot(results)

```

Finally, the average MSE, MAE and R-squared metrics were gathered by looking at their respective average values per model.

```{r}
#| label: Summary table
#| code-summary: "Summarise model performance metrics"

# Data taken from individual model statistics over the k-folded results (e.g. average RMSE).
# Data can be accessed by running the following code:
# summary(results)

models <- data.frame(
  model       = c("LM", "KNN", "RF"),
  RMSE        = c(0.92, 0.91, 0.87),
  MAE         = c(0.74, 0.72, 0.67),
  Rsquared    = c(0.18, 0.17, 0.25)
)

kbl(models, booktabs = T) %>%
    kable_styling(latex_options = args) 
```

Using the above data, the performance of three different supervised learning models were compared: Linear Regression with stepwise feature selection, K-Nearest Neighbours (KNN), and Random Forest. To ensure fair and robust comparisons, 10-fold cross-validation was utilised across all models. This technique helps avoid overfitting by training the models on different data splits and validating them on unseen partitions. 

From the boxplot visualisation and summary table, it was observed that Random Forest (RF) consistently achieved the lowest mean absolute error (MAE), with an average value of approximately 0.67. KNN also performed relatively well, with an average MAE of 0.72. Linear Regression (LM) had the highest average MAE at 0.74 and exhibited a significantly larger variance, indicating that on average it made larger errors in its predictions compared to the other two models. The RMSE results align closely with the MAE results. Random Forest (RF) achieved the lowest average RMSE of approximately 0.87, followed by KNN at 0.91, and Linear Regression (LM) with the highest RMSE at 0.92. This confirms that Random Forest made the smallest errors overall, while Linear Regression struggled with larger prediction errors. R-squared measures the proportion of variance in the dependent variable that is predictable from the independent variables. Here again, Random Forest (RF) outperformed the other models, with a average R-squared of 0.25, indicating that it explains the highest proportion of variance in the target variable. Linear Regression (LM)  had an average R-squared of 0.17, while KNN had the lowest at 0.18, suggesting that it explains the least amount of variance in the data.

The cross-validation results clearly show that the Random Forest model achieved the lowest average RMSE, making it the best-performing model. The KNN model came in second, while Linear Regression had the highest RMSE, indicating it performed worse compared to the other models. These results are visually summarised in the boxplot, where the RMSE and MAE distributions of each model across the folds can be observed.


The superior performance of the Random Forest model can be attributed to its ability to capture complex, non-linear relationships between features while controlling for overfitting through ensemble learning. Meanwhile, the KNN model also performed reasonably well, benefiting from the z-score standardization applied to ensure that all features contributed equally to distance calculations. In contrast, the Linear Regression model's simplicity and assumptions of linear relationships between the variables might have limited its ability to handle the complex feature interactions present in the data.

# Chosen model

After thoroughly comparing the performance of all three models, the Random Forest model was selected as the final model. The cross-validation results showed that Random Forest consistently achieved the best performance in terms of both accuracy (MAE and RMSE) and the ability to explain variance in the data (R-squared).
Random Forest’s ensemble learning approach, which aggregates multiple decision trees, allows it to handle complex interactions between variables and reduce overfitting, making it particularly well-suited for this task. Although KNN also showed competitive performance, the lower variability and superior accuracy of Random Forest make it the most reliable choice.

Having chosen the random forest model, predictions were made on the test data using the code below. They have been saved in the `predictions.rds` file. 

```{r}
#| label: Prediction test data
#| code-summary: "Make predictions on test data"
#| message: false

# Make predictions
predictions <- predict(model_rf, data_test)

# Save predictions
write_rds(predictions, "data/predictions.rds")
```

# Team member contributions

Write down what each team member contributed to the project.

- Nisse Hermsen: contribution to collaborative discussion on the modeling and comparison strategy, project setup, consistent folding with seed, model comparison boxplot and table, data normalisation using z-score, missing data check and description, draft for gridded histrograms, code-block folding and naming, grammar check and pronoun consistency
- Isabelle de Wolf: contribution to collaborative discussion on the modeling and comparison strategy, implementation of feature selection in linear regression, description of modeling and comparison strategy, creation and description of the bar graphs and histograms in variable distribution section, ...
- Agnieszka Kubica: contribution to collaborative discussion on the modeling and comparison strategy, analysis of variable types, creation of the variable table,  creation and description of the correlation plot, prepossessing decisions, part of the model creation description, description of the comparison strategy, justification of casting data types in the pre-processing, final final proof-read and rewrite
- Sara Sakhi: contribution to collaborative discussion on the modeling and comparison strategy, analysis and descriptions in the model comparison and chosen model sections
- Celis Tittse: contribution to collaborative discussion on the modeling and comparison strategy, first 3 paragraphs of data description, text in the model creation section, particularly justification of hypterparameters 


# References

<div id="refs"></div>
